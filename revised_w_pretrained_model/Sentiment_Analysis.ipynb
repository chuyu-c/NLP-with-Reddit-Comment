{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIFmLTekNHnQ"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SENTIMENT_EN.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqhrT47FOMLd"
      },
      "source": [
        "# **Connect to GCP Storage Bucket to Read Reddit Comment Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "qqt31pZ97WZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6cj70_Q8HQr",
        "outputId": "d973b9c4-196c-438d-c7f6-86b4dde35707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  68567      0 --:--:-- --:--:-- --:--:-- 68567\n",
            "OK\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 12.5 MB of archives.\n",
            "After this operation, 28.5 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.39.2_amd64.deb ...\n",
            "Unpacking gcsfuse (0.39.2) ...\n",
            "Setting up gcsfuse (0.39.2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import contextmanager\n",
        "import sys, os\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:  \n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n"
      ],
      "metadata": {
        "id": "JkCuUucJQrQY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with suppress_stdout():\n",
        "  !curl https://sdk.cloud.google.com | bash"
      ],
      "metadata": {
        "id": "LrtN5VDd_sp6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud init\n",
        "# Region & zone -> us-central1-a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kma5Q-9wAANG",
        "outputId": "9b32b103-0b36-4db3-a5dd-cb841a91179c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: chuyuc77@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for this \n",
            "configuration:\n",
            " [1] chuyuc77@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [chuyuc77@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] nimble-net-337716\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list item):  1\n",
            "\n",
            "Your current project has been set to: [nimble-net-337716].\n",
            "\n",
            "Do you want to configure a default Compute Region and Zone? (Y/n)?  y\n",
            "\n",
            "Which Google Compute Engine zone would you like to use as project default?\n",
            "If you do not specify a zone via a command line flag while working with Compute \n",
            "Engine resources, the default is assumed.\n",
            " [1] us-east1-b\n",
            " [2] us-east1-c\n",
            " [3] us-east1-d\n",
            " [4] us-east4-c\n",
            " [5] us-east4-b\n",
            " [6] us-east4-a\n",
            " [7] us-central1-c\n",
            " [8] us-central1-a\n",
            " [9] us-central1-f\n",
            " [10] us-central1-b\n",
            " [11] us-west1-b\n",
            " [12] us-west1-c\n",
            " [13] us-west1-a\n",
            " [14] europe-west4-a\n",
            " [15] europe-west4-b\n",
            " [16] europe-west4-c\n",
            " [17] europe-west1-b\n",
            " [18] europe-west1-d\n",
            " [19] europe-west1-c\n",
            " [20] europe-west3-c\n",
            " [21] europe-west3-a\n",
            " [22] europe-west3-b\n",
            " [23] europe-west2-c\n",
            " [24] europe-west2-b\n",
            " [25] europe-west2-a\n",
            " [26] asia-east1-b\n",
            " [27] asia-east1-a\n",
            " [28] asia-east1-c\n",
            " [29] asia-southeast1-b\n",
            " [30] asia-southeast1-a\n",
            " [31] asia-southeast1-c\n",
            " [32] asia-northeast1-b\n",
            " [33] asia-northeast1-c\n",
            " [34] asia-northeast1-a\n",
            " [35] asia-south1-c\n",
            " [36] asia-south1-b\n",
            " [37] asia-south1-a\n",
            " [38] australia-southeast1-b\n",
            " [39] australia-southeast1-c\n",
            " [40] australia-southeast1-a\n",
            " [41] southamerica-east1-b\n",
            " [42] southamerica-east1-c\n",
            " [43] southamerica-east1-a\n",
            " [44] asia-east2-a\n",
            " [45] asia-east2-b\n",
            " [46] asia-east2-c\n",
            " [47] asia-northeast2-a\n",
            " [48] asia-northeast2-b\n",
            " [49] asia-northeast2-c\n",
            " [50] asia-northeast3-a\n",
            "Did not print [39] options.\n",
            "Too many options [89]. Enter \"list\" at prompt to print choices fully.\n",
            "Please enter numeric choice or text value (must exactly match list item):  us-central1-a\n",
            "\n",
            "Your project default Compute Engine zone has been set to [us-central1-a].\n",
            "You can change it by running [gcloud config set compute/zone NAME].\n",
            "\n",
            "Your project default Compute Engine region has been set to [us-central1].\n",
            "You can change it by running [gcloud config set compute/region NAME].\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use chuyuc77@gmail.com by default\n",
            "* Commands will reference project `nimble-net-337716` by default\n",
            "* Compute Engine commands will use region `us-central1` by default\n",
            "* Compute Engine commands will use zone `us-central1-a` by default\n",
            "\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'nimble-net-337716'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNN5LeOXBj3V",
        "outputId": "54386dd2-f6de-41ce-c189-37a12533f36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "gs://databricks-627996572354853/\n",
            "gs://databricks-627996572354853-system/\n",
            "gs://dataproc-staging-us-central1-382293053940-zruhwxhi/\n",
            "gs://dataproc-temp-us-central1-382293053940-fjgbsxkw/\n",
            "gs://karglobal/\n",
            "gs://reddit-data1/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bucket_name = 'reddit-data1'\n",
        "\n",
        "# !gsutil -m cp -r gs://{bucket_name}/test.csv  . \n",
        "# !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 'reddit-data1' data"
      ],
      "metadata": {
        "id": "-Dpp2pW9ANR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from google.cloud.storage import client\n",
        "import io\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import json\n",
        "import filecmp"
      ],
      "metadata": {
        "id": "kEBpxi0N96t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_ACCOUNT = json.loads(r\"\"\"{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"nimble-net-337716\",\n",
        "  \"private_key_id\": \"10a4e090c5bc87fa85363e6df2907ab0f631c401\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCiNlKr4xf9fnPQ\\n+haxniJwwD+Khk30VsVO5wE6BHO+Jy5yQO1vPk/DeWAflQ/kx2xgZgUmrKM35w3o\\nw1FTA8NGi2y13gFyHkhxarPHRpy5y5wDxh/Btnut6nuzmw/EldqUJy0HkG93qWYM\\nCt3WoN1I+8dVcJckc1Q2igpHBNjd4IA6BA86DYeFqUPipYPSOvyDqhuUEhUcsGvF\\nFF+F+EbUAdEHqummQxVhOzwJZb0OWzCja2p/uyEC+EG807aJJ3h5X+bE9pRRSHW5\\nSt3gd5+v3RDpvZ+t/xowq5dyE8CZOI6knRXdaMhauuy9EGI7AQJ0Y7KyswmQ3Kfs\\nkPiIioNTAgMBAAECggEAAbJ1zPc+8gJXrdu1TxMWIyiFFwBEDTFh4RXI99VswemL\\nsoWmtko9mrrzXCvlaFcEdZcQ/hBCU5G8SITP81a6Z/Y0zfZOUjuqNbLlwfOlgiAf\\njWk46IBDQlTjXxlZNzemJh9UI5x0PhlB8xBOCofQ/jQm0r2vJyIkbb77T+Dnb/LR\\nZypT/n8jBznpCMAieorEgfa5hRdQdoAJSRfbtAZjgP6NFLpZ2KFc93adbF4bemHU\\n6ppdoP+p6epA7ZhxilFrlHmEwIoiwnLHRjJIZIJPUZ6Ax68+WblPZxwzC0U3Khah\\nL/g8Gretv6Uyd17Y8Vwvwjw7RZn4AFxzTOI22XJdKQKBgQDfMEG996Hb8I84S27A\\nqSzUpwxPQqDET8yaL5uuYGhBCpAyba6wj9nsVsKKVYheiESznpN1O2/nd87NMkZH\\n0nzbjEaLhU2n/PXPpsEoCdVWaT8H+o5ZudpBcrb8aRLGd0YNFWhtqg3FkyOJ0bng\\nHczUlo03dzNNXJqo/wq7TsAktQKBgQC6DzX/zuEVG4i2F/W2VoNfOCCKzNZp4nub\\nZgBMImq89pmKddtm5K0DZXp6k47FwvRG2A9AzFYVu+KTVmRvzFYF4R1g86kvUOLe\\ndYTiuCErD205hoLEqLpEfGSOSj6q/c7vGm9+5mZpNUE5I9jRcAqRRYLItGClTllF\\nPlnouWZU5wKBgDe7SB9Ur8FJD6piA7TuSbiMQiGkpJqAxrmVu81OW5oqNhmxk9aZ\\nBTTj8U4zz4qzglxL88xpXCcznptUu4IByJXWMGN0lRCmtQb2P/NsiS47t7aZVWSI\\naTzAlXrwDfUrIX3w45PGzAuTE2O90Clrp5NKSNcZ1+CwiBo9HYPqRUalAoGBAKhw\\nyt8qqITWZ38K0+zeB5L1mihuZxJ28pZt0okag21NBxXduuI81hEEFszPt2p8I2/b\\nfHuaQbjtXqMqjETWRW6PLWyvSpRGcw0YcVRbg9Oa5LQ2fT6SzgbgYpyaxH9Cxcub\\nMCM/bKmEh+a1+D0rZLW9qzgObIxbEOBqITVaWzUzAoGAW2Wul3gZEMbzE20koQE1\\nw9tHtdie60NiopMxYo93RDB35/4FYQLc01AXNcIMGugrAw6Tk/vMVjIPaBKXBz+E\\n/x7TjpnJ2qNS5Oh07+dc38wQc6Ijk6lxjjreap6jwWNZLf6zvwIzDlhb+TK9yq0h\\nyfwnsjju/jo0st2K/Hmeyzw=\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"reddit-data@nimble-net-337716.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"116478162933207172798\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/reddit-data%40nimble-net-337716.iam.gserviceaccount.com\"\n",
        "}\"\"\")\n",
        "\n",
        "BUCKET = \"reddit-data1\""
      ],
      "metadata": {
        "id": "BxuLjbO6974F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "    SERVICE_ACCOUNT,\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qvh97X1PFqH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = client.Client(\n",
        "    credentials=credentials,\n",
        "    project=credentials.project_id,\n",
        ")"
      ],
      "metadata": {
        "id": "OAljubWxFsRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_file(local_filename, remote_filename):\n",
        "    bucket = client.get_bucket(BUCKET)\n",
        "    blob = bucket.blob(remote_filename)\n",
        "    blob.upload_from_filename(local_filename)\n",
        "\n",
        "def download_file(local_filename, remote_filename):\n",
        "    bucket = client.get_bucket(BUCKET)\n",
        "    blob = bucket.blob(remote_filename)\n",
        "    blob.download_to_filename(local_filename)"
      ],
      "metadata": {
        "id": "BmYkXGLh7gkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test = pd.DataFrame(\n",
        "#     {\"col1\": [1,2,3],\n",
        "#      \"col2\": [4,5,6]}\n",
        "# ).to_csv(path_or_buf=\"/tmp/test.csv\")\n",
        "\n",
        "# save_file(\"/tmp/test.csv\",\"test.csv\")\n",
        "# download_file(\"/tmp/test2.csv\",\"test.csv\")\n",
        "# assert filecmp.cmp('/tmp/test.csv', '/tmp/test2.csv')"
      ],
      "metadata": {
        "id": "ekSa2pdF8asf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = download_file(\"/tmp/t.csv\",\"data\")\n",
        "# df = pd.read_csv(\"/tmp/t.csv\")\n",
        "# df.head(2)\n"
      ],
      "metadata": {
        "id": "wHHBVxM-G_TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koJZnWQNNPD_"
      },
      "source": [
        "## Colab Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6o8-g0tEqNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da93d570-142c-4efd-9a94-5b177a852e68"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# !bash colab.sh\n",
        "# -p is for pyspark\n",
        "# -s is for spark-nlp\n",
        "# !bash colab.sh -p 3.1.1 -s 3.0.1\n",
        "# by default they are set to the latest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-15 19:32:36--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-01-15 19:32:36--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-01-15 19:32:37--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1453 (1.4K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-15 19:32:37 (33.3 MB/s) - written to stdout [1453/1453]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.4.0\n",
            "Installing PySpark 3.0.3 and Spark NLP 3.4.0\n",
            "\u001b[K     |████████████████████████████████| 209.1 MB 62 kB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 70.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark import HiveContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
        "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
        "from pyspark.sql.types import StringType,IntegerType\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "2B99LVw1iHDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMmT9S6mE0ad"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3n4NloINS22"
      },
      "source": [
        "## 2. Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBXbY_vE2ss"
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = download_file(\"/tmp/t.csv\",\"data_sample\")\n",
        "df = spark.read \\\n",
        "    .option(\"delimiter\",\",\") \\\n",
        "    .option(\"multiLine\",\"true\") \\\n",
        "    .option(\"quote\", \"\\\"\")  \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
        "    .csv(\"/tmp/t.csv\",inferSchema=True, header=True)\n"
      ],
      "metadata": {
        "id": "jjMJ1Z0A8s-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wQXKHP3ktLU",
        "outputId": "f1a960ec-f674-4335-fe0b-c1b45e78b768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- body: string (nullable = true)\n",
            " |-- score_hidden: string (nullable = true)\n",
            " |-- archived: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- author_flair_text: string (nullable = true)\n",
            " |-- downs: string (nullable = true)\n",
            " |-- created_utc: string (nullable = true)\n",
            " |-- subreddit_id: string (nullable = true)\n",
            " |-- link_id: string (nullable = true)\n",
            " |-- parent_id: string (nullable = true)\n",
            " |-- score: string (nullable = true)\n",
            " |-- retrieved_on: string (nullable = true)\n",
            " |-- controversiality: string (nullable = true)\n",
            " |-- gilded: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- subreddit: string (nullable = true)\n",
            " |-- ups: string (nullable = true)\n",
            " |-- distinguished: string (nullable = true)\n",
            " |-- author_flair_css_class: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop rows with missing values\n",
        "df.dropna().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5PwYkCDITjb",
        "outputId": "cdf39ee7-45c6-406e-f25b-f06935b7fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert column type from string to integer\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df = df.withColumn(\"score\", F.col(\"score\").astype(IntegerType()))\n",
        "df = df.withColumn(\"ups\", F.col(\"ups\").astype(IntegerType()))"
      ],
      "metadata": {
        "id": "DXFlbuYL3Rt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable calculate percentage of upvotes\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df=df.withColumn(\"percent_upvotes\", col(\"ups\") / col(\"score\"))"
      ],
      "metadata": {
        "id": "bzUPc_Ie3XdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.select(['body','percent_upvotes','ups','score'])"
      ],
      "metadata": {
        "id": "fXcWtaa03daF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "df1.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P47Hj0xIj9g",
        "outputId": "0fb20612-073e-41cb-d06d-c512dd541aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 82.4 ms, sys: 10.5 ms, total: 93 ms\n",
            "Wall time: 13.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5839420"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jGaL9x3IiKJ",
        "outputId": "203042b3-a6c3-4b7e-e0e6-409d654c81aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+----+-----+\n",
            "|                body|percent_upvotes| ups|score|\n",
            "+--------------------+---------------+----+-----+\n",
            "|         works great|           null|null|    1|\n",
            "|           [deleted]|           null|null|    1|\n",
            "|H&amp;R Block has...|           null|null|    1|\n",
            "|This person is in...|           null|null|    1|\n",
            "|   About tree-fiddy.|           null|null|    1|\n",
            "+--------------------+---------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(['body']).show(5,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_602NZFqIqdq",
        "outputId": "5904b926-3a31-4aaf-e6fa-b720ab8f24e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------+\n",
            "|body                                                   |\n",
            "+-------------------------------------------------------+\n",
            "|works great                                            |\n",
            "|[deleted]                                              |\n",
            "|H&amp;R Block has a good estimator app on their website|\n",
            "|This person is in equador                              |\n",
            "|About tree-fiddy.                                      |\n",
            "+-------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comment Text Cleaning"
      ],
      "metadata": {
        "id": "Q94d758IlEvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##COnvert to lower\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "lower_udf =udf(lower,StringType())"
      ],
      "metadata": {
        "id": "06zo8OL8IuBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Remove nonAscii\n",
        "def strip_non_ascii(data_str):\n",
        "#''' Returns the string without non ASCII characters'''\n",
        "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
        "    return ''.join(stripped)\n",
        "# setup pyspark udf function\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n"
      ],
      "metadata": {
        "id": "fRprH31JIwln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "##Fix abbreviations\n",
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
        "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str"
      ],
      "metadata": {
        "id": "2xxCGqFnIyYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Remove punctuations mentions and alphanumeric characters\n",
        "def remove_features(data_str):\n",
        "# compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "# convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "# remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "# remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "# remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "# remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "# remove non a-z 0-9 characters and words shorter than 1 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "# remove unwanted space, *.split() will automatically split on\n",
        "# whitespace and discard duplicates, the \" \".join() joins the\n",
        "# resulting list into one string.\n",
        "    return \" \".join(cleaned_str.split())\n",
        "# setup pyspark udf function"
      ],
      "metadata": {
        "id": "3-kGq4tUI0rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Remove stop words\n",
        "def remove_stops(data_str):\n",
        "# expects a string\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stops:\n",
        "# rebuild cleaned_str\n",
        "            if list_pos == 0:\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            list_pos += 1\n",
        "    return cleaned_str\n",
        "    "
      ],
      "metadata": {
        "id": "P5RSop17I4p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-Speech Tagging\n",
        "def tag_and_remove(data_str):\n",
        "    cleaned_str = ' '\n",
        "# noun tags\n",
        "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
        "# adjectives\n",
        "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
        "# verbs\n",
        "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
        "# break string into 'words'\n",
        "    text = data_str.split()\n",
        "# tag the text and keep only those with the right tags\n",
        "    tagged_text = pos_tag(text)\n",
        "    for tagged_word in tagged_text:\n",
        "        if tagged_word[1] in nltk_tags:\n",
        "            cleaned_str += tagged_word[0] + ' '\n",
        "    return cleaned_str"
      ],
      "metadata": {
        "id": "OS43HHFHI7C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Lemmatization\n",
        "def lemmatize(data_str):\n",
        "# expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ],
      "metadata": {
        "id": "HvzBRrlyI9f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_udf =udf(lower,StringType())\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
        "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
        "lemmatize_udf = udf(lemmatize, StringType())"
      ],
      "metadata": {
        "id": "VrIasazaI-Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jVJx5_wCJOpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.withColumn(\"lower_text\",lower_udf(df1[\"body\"]))\n",
        "df1 = df1.withColumn(\"text_non_asci\",fix_abbreviation_udf(df1[\"lower_text\"]))\n",
        "df1 = df1.withColumn(\"fixed_abbrev\",fix_abbreviation_udf(df1[\"text_non_asci\"]))\n",
        "df1 = df1.withColumn('removed_features',remove_features_udf(df1['fixed_abbrev']))\n"
      ],
      "metadata": {
        "id": "QLTYYbrnJBw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "# helper function to clean comments\n",
        "def processComment(comment):\n",
        "    # Remove HTML special entities (e.g. &amp;)\n",
        "    comment = re.sub(r'\\&\\w*;', '', str(comment))\n",
        "    #Convert @username to AT_USER\n",
        "    comment = re.sub('@[^\\s]+','', comment)\n",
        "    # Remove tickers\n",
        "    comment = re.sub(r'\\$\\w*', '', comment)\n",
        "    # To lowercase\n",
        "    comment = comment.lower()\n",
        "    # Remove hyperlinks\n",
        "    comment = re.sub(r'https?:\\/\\/.*\\/\\w*', '', comment)\n",
        "    # Remove hashtags\n",
        "    comment = re.sub(r'#\\w*', '', comment)\n",
        "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
        "    comment = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', comment)\n",
        "    # Remove words with 2 or fewer letters\n",
        "    comment = re.sub(r'\\b\\w{1,2}\\b', '', comment)\n",
        "    # Remove whitespace (including new line characters)\n",
        "    comment = re.sub(r'\\s\\s+', ' ', comment)\n",
        "    # Remove single space remaining at the front of the comment.\n",
        "    comment = comment.lstrip('') \n",
        "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
        "    comment = ''.join(c for c in comment if c <= '\\uffff') \n",
        "    return comment\n",
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "bZFaXrtqJBtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf,col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "func = udf(lambda x:processComment(x),StringType())\n",
        "df2 = df1.withColumn(\"text\",func(col(\"removed_features\")))"
      ],
      "metadata": {
        "id": "tx-8nmjPJBi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.drop(\"percent_upvotes\",'ups','score')\n",
        "df2.show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtu0CdPIJ1QA",
        "outputId": "4be1fc89-0553-40c9-866b-b3a88beecba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                body|          lower_text|       text_non_asci|        fixed_abbrev|    removed_features|                text|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|         works great|         works great|         works great|         works great|         works great|         works great|\n",
            "|           [deleted]|           [deleted]|           [deleted]|           [deleted]|             deleted|             deleted|\n",
            "|H&amp;R Block has...|h&amp;r block has...|h&amp;are block h...|h&amp;are block h...|h amp are block h...| amp are block ha...|\n",
            "|This person is in...|this person is in...|this person is in...|this person is in...|this person is in...| this person equador|\n",
            "|   About tree-fiddy.|   about tree-fiddy.|   about tree-fiddy.|   about tree-fiddy.|    about tree fiddy|    about tree fiddy|\n",
            "|Why is the entire...|why is the entire...|why is the entire...|why is the entire...|why is the entire...|why the entire th...|\n",
            "|SANITY is for the...|sanity is for the...|sanity is for the...|sanity is for the...|sanity is for the...| sanity for the weak|\n",
            "|Start a 401k or s...|start a 401k or s...|sta a 401k or sim...|sta a 401k or sim...|sta a k or simila...|sta similar futur...|\n",
            "|   yup, what he said|   yup, what he said|   yup, what he said|   yup, what he said|    yup what he said|       yup what said|\n",
            "|Oh ok. I seriousl...|oh ok. i seriousl...|oh ok. i seriousl...|oh ok. i seriousl...|oh ok i seriously...| seriously though...|\n",
            "|I'm very sorry fo...|i'm very sorry fo...|i'm very sorry fo...|i'm very sorry fo...|i m very sorry fo...| very sorry for t...|\n",
            "|     yep pretty much|     yep pretty much|     yep pretty much|     yep pretty much|     yep pretty much|     yep pretty much|\n",
            "|It’s rss lmao. Tr...|it’s rss lmao. tr...|it’s rss lmao. tr...|it’s rss lmao. tr...|rss lmao trippie ...|rss lmao trippie ...|\n",
            "|           [deleted]|           [deleted]|           [deleted]|           [deleted]|             deleted|             deleted|\n",
            "|                 Rip|                 rip|                 rip|                 rip|                 rip|                 rip|\n",
            "|                  \\*|                  \\*|                  \\*|                  \\*|                    |                    |\n",
            "|Took a mental hea...|took a mental hea...|took a mental hea...|took a mental hea...|took a mental hea...|took mental healt...|\n",
            "|And is torment th...|and is torment th...|and is torment th...|and is torment th...|and is torment th...|and torment the s...|\n",
            "|I lived in Greenf...|i lived in greenf...|i lived in greenf...|i lived in greenf...|i lived in greenf...| lived greenfield...|\n",
            "|&gt;Because they'...|&gt;because they'...|&gt;because they'...|&gt;because they'...|gt because they r...| because they sti...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_null(c):\n",
        "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
        "\n",
        "df2 = df2.select([to_null(c).alias(c) for c in df2.columns]).na.drop()\n",
        "# df2.show(20)"
      ],
      "metadata": {
        "id": "9R8VBcxpF7QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl8vsy03Np6C"
      },
      "source": [
        "###  Define Spark NLP pipleline - Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XxHWemdE5hX"
      },
      "source": [
        "# Select the DL model and re-run cells below\n",
        "\n",
        "#MODEL_NAME='sentimentdl_use_imdb'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiYxv0mOFIcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f971be35-b438-4698-dbb1-5bd25fed5741"
      },
      "source": [
        "MODEL_NAME='sentimentdl_use_twitter'\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentimentdl_use_twitter download started this may take some time.\n",
            "Approximate size to download 11.4 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rFCvvHnN3ox"
      },
      "source": [
        "### Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu9l7NI4N53g"
      },
      "source": [
        "# empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "# pipelineModel = nlpPipeline.fit(empty_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df2.select(['text'])\n",
        "pipelineModel = nlpPipeline.fit(data)\n",
        "#spark.createDataFrame(pd.DataFrame({\"text\":text_list}))\n",
        "result = pipelineModel.transform(data)"
      ],
      "metadata": {
        "id": "jaXGS-thKmou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OusXsM-BN2LF"
      },
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG6bwlHzaQTs",
        "outputId": "c967a60b-dd33-4d33-bd75-b30f362f1b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document| sentence_embeddings|           sentiment|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|         works great|[[document, 0, 10...|[[sentence_embedd...|[[category, 0, 10...|\n",
            "|             deleted|[[document, 0, 6,...|[[sentence_embedd...|[[category, 0, 6,...|\n",
            "| amp are block ha...|[[document, 0, 50...|[[sentence_embedd...|[[category, 0, 50...|\n",
            "| this person equador|[[document, 0, 18...|[[sentence_embedd...|[[category, 0, 18...|\n",
            "|    about tree fiddy|[[document, 0, 15...|[[sentence_embedd...|[[category, 0, 15...|\n",
            "|why the entire th...|[[document, 0, 48...|[[sentence_embedd...|[[category, 0, 48...|\n",
            "| sanity for the weak|[[document, 0, 18...|[[sentence_embedd...|[[category, 0, 18...|\n",
            "|sta similar futur...|[[document, 0, 41...|[[sentence_embedd...|[[category, 0, 41...|\n",
            "|       yup what said|[[document, 0, 12...|[[sentence_embedd...|[[category, 0, 12...|\n",
            "| seriously though...|[[document, 0, 33...|[[sentence_embedd...|[[category, 0, 33...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.show(10,truncate=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoWmwA8FLhcn",
        "outputId": "5cdfe297-476f-4128-eecf-8bc8e6c54db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|                                                                                            document|                                                                                 sentence_embeddings|                                                                                           sentiment|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                         works great|                                               [[document, 0, 10, works great, [sentence -> 0], []]]|[[sentence_embeddings, 0, 10, works great, [sentence -> 0, token -> works great, pieceId -> -1, i...|                [[category, 0, 10, positive, [sentence -> 0, positive -> 1.0, negative -> 0.0], []]]|\n",
            "|                                                                                             deleted|                                                    [[document, 0, 6, deleted, [sentence -> 0], []]]|[[sentence_embeddings, 0, 6, deleted, [sentence -> 0, token -> deleted, pieceId -> -1, isWordStar...|   [[category, 0, 6, negative, [sentence -> 0, positive -> 0.084529385, negative -> 0.9154706], []]]|\n",
            "|                                                  amp are block has good estimator app their website|       [[document, 0, 50,  amp are block has good estimator app their website, [sentence -> 0], []]]|[[sentence_embeddings, 0, 50,  amp are block has good estimator app their website, [sentence -> 0...|[[category, 0, 50, positive, [sentence -> 0, positive -> 0.99995494, negative -> 4.5064273E-5], []]]|\n",
            "|                                                                                 this person equador|                                       [[document, 0, 18, this person equador, [sentence -> 0], []]]|[[sentence_embeddings, 0, 18, this person equador, [sentence -> 0, token -> this person equador, ...|   [[category, 0, 18, negative, [sentence -> 0, positive -> 0.04781373, negative -> 0.9521863], []]]|\n",
            "|                                                                                    about tree fiddy|                                          [[document, 0, 15, about tree fiddy, [sentence -> 0], []]]|[[sentence_embeddings, 0, 15, about tree fiddy, [sentence -> 0, token -> about tree fiddy, pieceI...|   [[category, 0, 15, positive, [sentence -> 0, positive -> 0.6832606, negative -> 0.31673935], []]]|\n",
            "|                                                   why the entire thing just octaves lmao wait liszt|         [[document, 0, 48, why the entire thing just octaves lmao wait liszt, [sentence -> 0], []]]|[[sentence_embeddings, 0, 48, why the entire thing just octaves lmao wait liszt, [sentence -> 0, ...|  [[category, 0, 48, positive, [sentence -> 0, positive -> 0.9729006, negative -> 0.027099408], []]]|\n",
            "|                                                                                 sanity for the weak|                                       [[document, 0, 18, sanity for the weak, [sentence -> 0], []]]|[[sentence_embeddings, 0, 18, sanity for the weak, [sentence -> 0, token -> sanity for the weak, ...|[[category, 0, 18, negative, [sentence -> 0, positive -> 1.00694604E-4, negative -> 0.99989927], ...|\n",
            "|sta similar future you will thank you your young you can aggressive aggressive does not mean fool...|[[document, 0, 414, sta similar future you will thank you your young you can aggressive aggressiv...|[[sentence_embeddings, 0, 414, sta similar future you will thank you your young you can aggressiv...|               [[category, 0, 414, positive, [sentence -> 0, positive -> 1.0, negative -> 0.0], []]]|\n",
            "|                                                                                       yup what said|                                             [[document, 0, 12, yup what said, [sentence -> 0], []]]|[[sentence_embeddings, 0, 12, yup what said, [sentence -> 0, token -> yup what said, pieceId -> -...|       [[category, 0, 12, positive, [sentence -> 0, positive -> 1.0, negative -> 6.028981E-10], []]]|\n",
            "| seriously thought there was maybe something wrong with you haha yeah lot people like tell christ...|[[document, 0, 331,  seriously thought there was maybe something wrong with you haha yeah lot peo...|[[sentence_embeddings, 0, 331,  seriously thought there was maybe something wrong with you haha y...|               [[category, 0, 331, positive, [sentence -> 0, positive -> 1.0, negative -> 0.0], []]]|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzJSQhTnFix5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8c60d9-3b9f-4b36-d0f4-31f451a07f00"
      },
      "source": [
        "\n",
        "result_new = result.select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"document\"),\n",
        "        F.expr(\"cols['1']\").alias(\"sentiment\"))\n",
        "\n",
        "result_new.show(truncate=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+---------+\n",
            "|                                                                                            document|sentiment|\n",
            "+----------------------------------------------------------------------------------------------------+---------+\n",
            "|                                                                                         works great| positive|\n",
            "|                                                                                             deleted| negative|\n",
            "|                                                  amp are block has good estimator app their website| positive|\n",
            "|                                                                                 this person equador| negative|\n",
            "|                                                                                    about tree fiddy| positive|\n",
            "|                                                   why the entire thing just octaves lmao wait liszt| positive|\n",
            "|                                                                                 sanity for the weak| negative|\n",
            "|sta similar future you will thank you your young you can aggressive aggressive does not mean fool...| positive|\n",
            "|                                                                                       yup what said| positive|\n",
            "| seriously thought there was maybe something wrong with you haha yeah lot people like tell christ...| positive|\n",
            "| very sorry for the loss your dear buddy sure knew how much was loved you were lucky have each ot...| negative|\n",
            "|                                                                                     yep pretty much| positive|\n",
            "|                                         rss lmao trippie went jauns warehouse and picked some breds| negative|\n",
            "|                                                                                             deleted| negative|\n",
            "|                                                                                                 rip| negative|\n",
            "|     took mental health day and have gone the gym turns out mid morning wednesday retiree boomer day| negative|\n",
            "|                                                                        and torment the same torture| negative|\n",
            "| lived greenfield briefly few years back actually really like that town loving the warmth and the...| positive|\n",
            "| because they still based the world largest spyware android the operating system plays major role...| positive|\n",
            "|                      well then anyone else excited see what glitches the newest update brings with | positive|\n",
            "+----------------------------------------------------------------------------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LM4y5NCVoj5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}